{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eb2738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tqdm\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import keras functions\n",
    "# import tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, Input, Flatten, LSTM, Dropout, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# performance matrices\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, accuracy_score\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape, Lambda, Embedding\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "from keras.backend import expand_dims\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.regularizers import l1, l2\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8c1a1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history):\n",
    "    # learning curves of model accuracy\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9b4b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_for_embedding(fasta_file):\n",
    "    encodings = []\n",
    "    \n",
    "    # define universe of possible input values\n",
    "    alphabet = 'ARNDCQEGHILKMFPSTWYV-'\n",
    "    \n",
    "    # define a mapping of chars to integers\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "    \n",
    "    for seq_record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        data = seq_record.seq\n",
    "        for char in data:\n",
    "            if char not in alphabet:\n",
    "                return\n",
    "        integer_encoded = [char_to_int[char] for char in data]\n",
    "        encodings.append(integer_encoded)\n",
    "    encodings = np.array(encodings)\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f3b326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sequences to integer encoding, for embedding\n",
    "#test_positive_embedding = get_input_for_embedding('data/DeepSuccinylSite/fasta/test_positive_sites.fasta')\n",
    "#test_negative_embedding = get_input_for_embedding('data/DeepSuccinylSite/fasta/test_negative_sites.fasta')\n",
    "#train_positive_embedding = get_input_for_embedding('data/DeepSuccinylSite/fasta/positive_sites.fasta')\n",
    "#train_negative_embedding = get_input_for_embedding('data/DeepSuccinylSite/fasta/negative_sites.fasta')\n",
    "\n",
    "# convert sequences to integer encoding, for embedding\n",
    "test_positive_embedding = get_input_for_embedding('../data/test/fasta/test_positive_sites.fasta')\n",
    "test_negative_embedding = get_input_for_embedding('../data/test/fasta/test_negative_sites.fasta')\n",
    "train_positive_embedding = get_input_for_embedding('../data/train/fasta/positive_sites.fasta')\n",
    "train_negative_embedding = get_input_for_embedding('../data/train/fasta/negative_sites.fasta')\n",
    "\n",
    "# create labels\n",
    "train_positive_labels = np.ones(train_positive_embedding.shape[0])\n",
    "train_negative_labels = np.zeros(train_negative_embedding.shape[0])\n",
    "test_positive_labels = np.ones(test_positive_embedding.shape[0])\n",
    "test_negative_labels = np.zeros(test_negative_embedding.shape[0])\n",
    "\n",
    "# stack positive and negative data together\n",
    "X_train_full_embedding = np.vstack((train_positive_embedding,train_negative_embedding))\n",
    "X_test_embedding = np.vstack((test_positive_embedding,test_negative_embedding))\n",
    "y_train_full = np.concatenate((train_positive_labels, train_negative_labels), axis = 0)\n",
    "y_test = np.concatenate((test_positive_labels, test_negative_labels), axis = 0)\n",
    "\n",
    "#train_positive_pt5 = pd.read_csv(\"data/DeepSuccinylSite/features/full/train_positive_ProtT5-XL-UniRef50.csv\", header = None).iloc[:,2:]\n",
    "#train_negative_pt5 = pd.read_csv(\"data/DeepSuccinylSite/features/full/train_negative_ProtT5-XL-UniRef50.csv\", header = None).iloc[:,2:]\n",
    "#test_positive_pt5 = pd.read_csv(\"data/DeepSuccinylSite/features/full/test_positive_ProtT5-XL-UniRef50.csv\", header = None).iloc[:,2:]\n",
    "#test_negative_pt5 = pd.read_csv(\"data/DeepSuccinylSite/features/full/test_negative_ProtT5-XL-UniRef50.csv\", header = None).iloc[:,2:]\n",
    "\n",
    "train_positive_pt5 = pd.read_csv(\"../data/train/features/train_positive_ProtT5-XL-UniRef50.csv\", header = None).iloc[:,2:]\n",
    "train_negative_pt5 = pd.read_csv(\"../data/train/features/train_negative_ProtT5-XL-UniRef50.csv\", header = None).iloc[:,2:]\n",
    "test_positive_pt5 = pd.read_csv(\"../data/test/features/test_positive_ProtT5-XL-UniRef50.csv\", header = None).iloc[:,2:]\n",
    "test_negative_pt5 = pd.read_csv(\"../data/test/features/test_negative_ProtT5-XL-UniRef50.csv\", header = None).iloc[:,2:]\n",
    "\n",
    "\n",
    "# stack positive and negative data together\n",
    "X_train_pt5_full = np.vstack((train_positive_pt5,train_negative_pt5))\n",
    "X_test_pt5 = np.vstack((test_positive_pt5,test_negative_pt5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7389589c-1d7c-4a57-951d-31beaf477cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle X and y together\n",
    "# X_train_pt5_full, X_train_full_embedding, y_train = shuffle(X_train_pt5_full, X_train_full_embedding, y_train_full)\n",
    "# X_test_pt5, X_test_embedding, y_test = shuffle(X_test_pt5, X_test_embedding, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e641a077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.summary()\n",
    "# plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a7fbd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def CNN_Embedding():\n",
    "    # Embedding\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(256, 21, input_length=33))\n",
    "    model.add(Lambda(lambda x: K.expand_dims(x, 3)))\n",
    "    model.add(Conv2D(32, kernel_size=(17, 3), activation = 'relu', kernel_initializer='he_normal', padding = 'VALID'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(16, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model'''\n",
    "\n",
    "#import tensorflow as tf\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Layer, GlobalAveragePooling1D, Input, GlobalAveragePooling2D\n",
    "#from tensorflow.keras.layers import Dense, Embedding\n",
    "#from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, dropout=rate)\n",
    "        self.ffn = Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        return config\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output, attention_scores = self.att(inputs, inputs, inputs, return_attention_scores=True, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output), attention_scores\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim, mask_zero=True)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        return config\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "def embedding_model():\n",
    "    # transformer based embedding model\n",
    "    vocab_size = 21\n",
    "    embed_dim = 128 #config[\"embedding_dim\"]\n",
    "    ff_dim = 128 #config[\"feed_forward_dim\"]\n",
    "    max_len = 33 #config[\"maximum_path_length\"]\n",
    "    dropout = 0.2 #config[\"dropout\"]\n",
    "    n_heads = 8\n",
    "\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(max_len, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, n_heads, ff_dim)\n",
    "    x, weights = transformer_block(x)\n",
    "    print(x.shape, weights.shape)\n",
    "    x = GlobalAveragePooling1D()(x) #GlobalAveragePooling1D\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(ff_dim, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(16, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "    return Model(inputs=inputs, outputs=[outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0855e40f-102c-43d4-a8df-d83fc94c8fbc",
   "metadata": {},
   "source": [
    "### Iterated 10-Fold CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d8f13cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 33, 128) (None, 8, 33, 33)\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 33)]              0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, 33, 128)          6912      \n",
      " g_6 (TokenAndPositionEmbedd                                     \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " transformer_block_6 (Transf  ((None, 33, 128),        561024    \n",
      " ormerBlock)                  (None, 8, 33, 33))                 \n",
      "                                                                 \n",
      " global_average_pooling1d_6   (None, 128)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 16)                2064      \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 16)                0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 586,529\n",
      "Trainable params: 586,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "268/268 [==============================] - 5s 12ms/step - loss: 0.6949 - accuracy: 0.5035 - val_loss: 0.6923 - val_accuracy: 0.5032\n",
      "Epoch 2/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.6939 - accuracy: 0.4995 - val_loss: 0.6931 - val_accuracy: 0.5032\n",
      "Epoch 3/100\n",
      "268/268 [==============================] - 3s 11ms/step - loss: 0.6932 - accuracy: 0.5042 - val_loss: 0.6921 - val_accuracy: 0.5737\n",
      "Epoch 4/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.6882 - accuracy: 0.5485 - val_loss: 0.6752 - val_accuracy: 0.5863\n",
      "Epoch 5/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.6822 - accuracy: 0.5733 - val_loss: 0.6775 - val_accuracy: 0.5926\n",
      "Epoch 6/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.6812 - accuracy: 0.5709 - val_loss: 0.6701 - val_accuracy: 0.5811\n",
      "Epoch 7/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.6754 - accuracy: 0.5839 - val_loss: 0.6641 - val_accuracy: 0.5916\n",
      "Epoch 8/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.6767 - accuracy: 0.5750 - val_loss: 0.6624 - val_accuracy: 0.5926\n",
      "Epoch 9/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.6732 - accuracy: 0.5803 - val_loss: 0.6662 - val_accuracy: 0.5926\n",
      "Epoch 10/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.6670 - accuracy: 0.5940 - val_loss: 0.6522 - val_accuracy: 0.6063\n",
      "Epoch 11/100\n",
      "268/268 [==============================] - 3s 11ms/step - loss: 0.6570 - accuracy: 0.6066 - val_loss: 0.6385 - val_accuracy: 0.6295\n",
      "Epoch 12/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.6409 - accuracy: 0.6294 - val_loss: 0.6259 - val_accuracy: 0.6453\n",
      "Epoch 13/100\n",
      "268/268 [==============================] - 3s 11ms/step - loss: 0.6178 - accuracy: 0.6628 - val_loss: 0.5968 - val_accuracy: 0.6853\n",
      "Epoch 14/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.6062 - accuracy: 0.6782 - val_loss: 0.5990 - val_accuracy: 0.6737\n",
      "Epoch 15/100\n",
      "268/268 [==============================] - 3s 11ms/step - loss: 0.6029 - accuracy: 0.6856 - val_loss: 0.5807 - val_accuracy: 0.7053\n",
      "Epoch 16/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5929 - accuracy: 0.6961 - val_loss: 0.5882 - val_accuracy: 0.7000\n",
      "Epoch 17/100\n",
      "268/268 [==============================] - 3s 11ms/step - loss: 0.5908 - accuracy: 0.6975 - val_loss: 0.5759 - val_accuracy: 0.7063\n",
      "Epoch 18/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5829 - accuracy: 0.7025 - val_loss: 0.5816 - val_accuracy: 0.6958\n",
      "Epoch 19/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5749 - accuracy: 0.7083 - val_loss: 0.5784 - val_accuracy: 0.7042\n",
      "Epoch 20/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5662 - accuracy: 0.7100 - val_loss: 0.5710 - val_accuracy: 0.7221\n",
      "Epoch 21/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5631 - accuracy: 0.7140 - val_loss: 0.5736 - val_accuracy: 0.7116\n",
      "Epoch 22/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5579 - accuracy: 0.7169 - val_loss: 0.5649 - val_accuracy: 0.7011\n",
      "Epoch 23/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5517 - accuracy: 0.7272 - val_loss: 0.5658 - val_accuracy: 0.7053\n",
      "Epoch 24/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5465 - accuracy: 0.7296 - val_loss: 0.5457 - val_accuracy: 0.7211\n",
      "Epoch 25/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5358 - accuracy: 0.7355 - val_loss: 0.5625 - val_accuracy: 0.7021\n",
      "Epoch 26/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5364 - accuracy: 0.7384 - val_loss: 0.5658 - val_accuracy: 0.7126\n",
      "Epoch 27/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5329 - accuracy: 0.7373 - val_loss: 0.5495 - val_accuracy: 0.7179\n",
      "Epoch 28/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5294 - accuracy: 0.7395 - val_loss: 0.5464 - val_accuracy: 0.7189\n",
      "Epoch 29/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5267 - accuracy: 0.7399 - val_loss: 0.5432 - val_accuracy: 0.7316\n",
      "Epoch 30/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5248 - accuracy: 0.7462 - val_loss: 0.5457 - val_accuracy: 0.7284\n",
      "Epoch 31/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5262 - accuracy: 0.7401 - val_loss: 0.5586 - val_accuracy: 0.7147\n",
      "Epoch 32/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5217 - accuracy: 0.7427 - val_loss: 0.5601 - val_accuracy: 0.6989\n",
      "Epoch 33/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5152 - accuracy: 0.7492 - val_loss: 0.5507 - val_accuracy: 0.7263\n",
      "Epoch 34/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5117 - accuracy: 0.7537 - val_loss: 0.5525 - val_accuracy: 0.7221\n",
      "Epoch 35/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5118 - accuracy: 0.7527 - val_loss: 0.5672 - val_accuracy: 0.7242\n",
      "Epoch 36/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5083 - accuracy: 0.7594 - val_loss: 0.5528 - val_accuracy: 0.7116\n",
      "Epoch 37/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5047 - accuracy: 0.7572 - val_loss: 0.5604 - val_accuracy: 0.7263\n",
      "Epoch 38/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5036 - accuracy: 0.7604 - val_loss: 0.5537 - val_accuracy: 0.7211\n",
      "Epoch 39/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4992 - accuracy: 0.7563 - val_loss: 0.5688 - val_accuracy: 0.7000\n",
      "Epoch 40/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4974 - accuracy: 0.7620 - val_loss: 0.5817 - val_accuracy: 0.7021\n",
      "Epoch 41/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4954 - accuracy: 0.7603 - val_loss: 0.5554 - val_accuracy: 0.7105\n",
      "Epoch 42/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4898 - accuracy: 0.7594 - val_loss: 0.5658 - val_accuracy: 0.7095\n",
      "Epoch 43/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4897 - accuracy: 0.7636 - val_loss: 0.5578 - val_accuracy: 0.7011\n",
      "Epoch 44/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4897 - accuracy: 0.7690 - val_loss: 0.5887 - val_accuracy: 0.7126\n",
      "Epoch 45/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4855 - accuracy: 0.7644 - val_loss: 0.5788 - val_accuracy: 0.7168\n",
      "Epoch 46/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4788 - accuracy: 0.7700 - val_loss: 0.5545 - val_accuracy: 0.7084\n",
      "Epoch 47/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4760 - accuracy: 0.7746 - val_loss: 0.5596 - val_accuracy: 0.7137\n",
      "Epoch 48/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4743 - accuracy: 0.7732 - val_loss: 0.5780 - val_accuracy: 0.7095\n",
      "Epoch 49/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4721 - accuracy: 0.7710 - val_loss: 0.5658 - val_accuracy: 0.7179\n",
      "Epoch 50/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4680 - accuracy: 0.7769 - val_loss: 0.5615 - val_accuracy: 0.7189\n",
      "Epoch 51/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4612 - accuracy: 0.7799 - val_loss: 0.5834 - val_accuracy: 0.7147\n",
      "Epoch 52/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4635 - accuracy: 0.7793 - val_loss: 0.5981 - val_accuracy: 0.7042\n",
      "Epoch 53/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4653 - accuracy: 0.7782 - val_loss: 0.5917 - val_accuracy: 0.7105\n",
      "Epoch 54/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4533 - accuracy: 0.7795 - val_loss: 0.5769 - val_accuracy: 0.7242\n",
      "Epoch 55/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4562 - accuracy: 0.7865 - val_loss: 0.5825 - val_accuracy: 0.7084\n",
      "Epoch 56/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4499 - accuracy: 0.7910 - val_loss: 0.5858 - val_accuracy: 0.7126\n",
      "Epoch 57/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4450 - accuracy: 0.7937 - val_loss: 0.5808 - val_accuracy: 0.7074\n",
      "Epoch 58/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4357 - accuracy: 0.7927 - val_loss: 0.5840 - val_accuracy: 0.7147\n",
      "Epoch 59/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4375 - accuracy: 0.7955 - val_loss: 0.5973 - val_accuracy: 0.7116\n",
      "Epoch 60/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4338 - accuracy: 0.7935 - val_loss: 0.5620 - val_accuracy: 0.7211\n",
      "Epoch 61/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4283 - accuracy: 0.8015 - val_loss: 0.5930 - val_accuracy: 0.7137\n",
      "Epoch 62/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4308 - accuracy: 0.8026 - val_loss: 0.6237 - val_accuracy: 0.6958\n",
      "Epoch 63/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4227 - accuracy: 0.8071 - val_loss: 0.6028 - val_accuracy: 0.7116\n",
      "Epoch 64/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4197 - accuracy: 0.8061 - val_loss: 0.6187 - val_accuracy: 0.7105\n",
      "Epoch 65/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4204 - accuracy: 0.8109 - val_loss: 0.6123 - val_accuracy: 0.7021\n",
      "Epoch 66/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4129 - accuracy: 0.8114 - val_loss: 0.6111 - val_accuracy: 0.7126\n",
      "Epoch 67/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4173 - accuracy: 0.8055 - val_loss: 0.6424 - val_accuracy: 0.7126\n",
      "Epoch 68/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4083 - accuracy: 0.8100 - val_loss: 0.6302 - val_accuracy: 0.7116\n",
      "Epoch 69/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4068 - accuracy: 0.8135 - val_loss: 0.6216 - val_accuracy: 0.7105\n",
      "Epoch 70/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4008 - accuracy: 0.8189 - val_loss: 0.6026 - val_accuracy: 0.7189\n",
      "Epoch 71/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.3959 - accuracy: 0.8165 - val_loss: 0.6958 - val_accuracy: 0.7084\n",
      "Epoch 72/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.3976 - accuracy: 0.8209 - val_loss: 0.6871 - val_accuracy: 0.7158\n",
      "Epoch 73/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4016 - accuracy: 0.8174 - val_loss: 0.6193 - val_accuracy: 0.7168\n",
      "Epoch 74/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.3920 - accuracy: 0.8151 - val_loss: 0.6821 - val_accuracy: 0.7179\n",
      "Epoch 75/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.3926 - accuracy: 0.8165 - val_loss: 0.7237 - val_accuracy: 0.7126\n",
      "Epoch 76/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.3882 - accuracy: 0.8236 - val_loss: 0.6257 - val_accuracy: 0.7232\n",
      "Epoch 77/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.3757 - accuracy: 0.8243 - val_loss: 0.7019 - val_accuracy: 0.7189\n",
      "Epoch 78/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.3845 - accuracy: 0.8204 - val_loss: 0.6901 - val_accuracy: 0.7211\n",
      "Epoch 79/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.3701 - accuracy: 0.8309 - val_loss: 0.7268 - val_accuracy: 0.7074\n",
      "Epoch 80/100\n",
      "268/268 [==============================] - 3s 11ms/step - loss: 0.3731 - accuracy: 0.8282 - val_loss: 0.6822 - val_accuracy: 0.7274\n",
      "Epoch 81/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.3700 - accuracy: 0.8330 - val_loss: 0.6768 - val_accuracy: 0.7053\n",
      "Epoch 82/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.3715 - accuracy: 0.8279 - val_loss: 0.6477 - val_accuracy: 0.7211\n",
      "Epoch 83/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.3619 - accuracy: 0.8339 - val_loss: 0.6746 - val_accuracy: 0.7189\n",
      "Epoch 84/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.3660 - accuracy: 0.8330 - val_loss: 0.6502 - val_accuracy: 0.7126\n",
      "Epoch 85/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.3587 - accuracy: 0.8350 - val_loss: 0.6922 - val_accuracy: 0.7179\n",
      "Epoch 86/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.3666 - accuracy: 0.8376 - val_loss: 0.6379 - val_accuracy: 0.7158\n",
      "Epoch 87/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.3552 - accuracy: 0.8430 - val_loss: 0.6599 - val_accuracy: 0.7242\n",
      "Epoch 88/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.3502 - accuracy: 0.8445 - val_loss: 0.7105 - val_accuracy: 0.7137\n",
      "Epoch 89/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.3498 - accuracy: 0.8454 - val_loss: 0.6812 - val_accuracy: 0.7221\n",
      "Epoch 90/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.3479 - accuracy: 0.8444 - val_loss: 0.6937 - val_accuracy: 0.7063\n",
      "Epoch 91/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.3556 - accuracy: 0.8430 - val_loss: 0.7250 - val_accuracy: 0.7158\n",
      "Epoch 92/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.3435 - accuracy: 0.8470 - val_loss: 0.7232 - val_accuracy: 0.7095\n",
      "Epoch 93/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.3415 - accuracy: 0.8499 - val_loss: 0.7355 - val_accuracy: 0.7232\n",
      "Epoch 94/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.3315 - accuracy: 0.8568 - val_loss: 0.7574 - val_accuracy: 0.7200\n",
      "Epoch 95/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.3365 - accuracy: 0.8485 - val_loss: 0.7100 - val_accuracy: 0.7242\n",
      "Epoch 96/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.3309 - accuracy: 0.8539 - val_loss: 0.8525 - val_accuracy: 0.7084\n",
      "Epoch 97/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.3330 - accuracy: 0.8539 - val_loss: 0.7361 - val_accuracy: 0.7074\n",
      "Epoch 98/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.3273 - accuracy: 0.8572 - val_loss: 0.7379 - val_accuracy: 0.7074\n",
      "Epoch 99/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.3238 - accuracy: 0.8582 - val_loss: 0.7584 - val_accuracy: 0.7000\n",
      "Epoch 100/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.3193 - accuracy: 0.8560 - val_loss: 0.8034 - val_accuracy: 0.7116\n",
      "0.7115789473684211 0.42567924041876226 0.760593220338983 0.6631799163179917\n",
      "(None, 33, 128) (None, 8, 33, 33)\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 33)]              0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, 33, 128)          6912      \n",
      " g_7 (TokenAndPositionEmbedd                                     \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " transformer_block_7 (Transf  ((None, 33, 128),        561024    \n",
      " ormerBlock)                  (None, 8, 33, 33))                 \n",
      "                                                                 \n",
      " global_average_pooling1d_7   (None, 128)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 16)                2064      \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 16)                0         \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 586,529\n",
      "Trainable params: 586,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "268/268 [==============================] - 5s 12ms/step - loss: 0.6987 - accuracy: 0.5032 - val_loss: 0.6928 - val_accuracy: 0.5484\n",
      "Epoch 2/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.6920 - accuracy: 0.5254 - val_loss: 0.6930 - val_accuracy: 0.5084\n",
      "Epoch 3/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.6881 - accuracy: 0.5429 - val_loss: 0.6878 - val_accuracy: 0.5568\n",
      "Epoch 4/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.6816 - accuracy: 0.5668 - val_loss: 0.6831 - val_accuracy: 0.5611\n",
      "Epoch 5/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.6774 - accuracy: 0.5794 - val_loss: 0.6856 - val_accuracy: 0.5568\n",
      "Epoch 6/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.6797 - accuracy: 0.5676 - val_loss: 0.6821 - val_accuracy: 0.5611\n",
      "Epoch 7/100\n",
      "268/268 [==============================] - 3s 11ms/step - loss: 0.6748 - accuracy: 0.5798 - val_loss: 0.6793 - val_accuracy: 0.5684\n",
      "Epoch 8/100\n",
      "268/268 [==============================] - 3s 11ms/step - loss: 0.6746 - accuracy: 0.5852 - val_loss: 0.6789 - val_accuracy: 0.5747\n",
      "Epoch 9/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.6656 - accuracy: 0.5983 - val_loss: 0.6539 - val_accuracy: 0.6116\n",
      "Epoch 10/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.6538 - accuracy: 0.6094 - val_loss: 0.6600 - val_accuracy: 0.5916\n",
      "Epoch 11/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.6435 - accuracy: 0.6228 - val_loss: 0.6511 - val_accuracy: 0.6305\n",
      "Epoch 12/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.6386 - accuracy: 0.6367 - val_loss: 0.6536 - val_accuracy: 0.6379\n",
      "Epoch 13/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.6343 - accuracy: 0.6363 - val_loss: 0.6367 - val_accuracy: 0.6558\n",
      "Epoch 14/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.6189 - accuracy: 0.6533 - val_loss: 0.6293 - val_accuracy: 0.6453\n",
      "Epoch 15/100\n",
      "268/268 [==============================] - 3s 11ms/step - loss: 0.6094 - accuracy: 0.6758 - val_loss: 0.6185 - val_accuracy: 0.6768\n",
      "Epoch 16/100\n",
      "268/268 [==============================] - 3s 11ms/step - loss: 0.5993 - accuracy: 0.6828 - val_loss: 0.6033 - val_accuracy: 0.6958\n",
      "Epoch 17/100\n",
      "268/268 [==============================] - 3s 11ms/step - loss: 0.5905 - accuracy: 0.6932 - val_loss: 0.6025 - val_accuracy: 0.6968\n",
      "Epoch 18/100\n",
      "268/268 [==============================] - 3s 11ms/step - loss: 0.5769 - accuracy: 0.6984 - val_loss: 0.5899 - val_accuracy: 0.6958\n",
      "Epoch 19/100\n",
      "268/268 [==============================] - 3s 11ms/step - loss: 0.5714 - accuracy: 0.7059 - val_loss: 0.5839 - val_accuracy: 0.7000\n",
      "Epoch 20/100\n",
      "268/268 [==============================] - 3s 11ms/step - loss: 0.5652 - accuracy: 0.7135 - val_loss: 0.5829 - val_accuracy: 0.7158\n",
      "Epoch 21/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5668 - accuracy: 0.7172 - val_loss: 0.5802 - val_accuracy: 0.6968\n",
      "Epoch 22/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5561 - accuracy: 0.7239 - val_loss: 0.5849 - val_accuracy: 0.7095\n",
      "Epoch 23/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5547 - accuracy: 0.7282 - val_loss: 0.5756 - val_accuracy: 0.7168\n",
      "Epoch 24/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5425 - accuracy: 0.7324 - val_loss: 0.5696 - val_accuracy: 0.7253\n",
      "Epoch 25/100\n",
      "268/268 [==============================] - 3s 11ms/step - loss: 0.5376 - accuracy: 0.7352 - val_loss: 0.5636 - val_accuracy: 0.7337\n",
      "Epoch 26/100\n",
      "268/268 [==============================] - 3s 11ms/step - loss: 0.5264 - accuracy: 0.7410 - val_loss: 0.5494 - val_accuracy: 0.7389\n",
      "Epoch 27/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5253 - accuracy: 0.7429 - val_loss: 0.5600 - val_accuracy: 0.7274\n",
      "Epoch 28/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5146 - accuracy: 0.7542 - val_loss: 0.5486 - val_accuracy: 0.7358\n",
      "Epoch 29/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5167 - accuracy: 0.7520 - val_loss: 0.5576 - val_accuracy: 0.7095\n",
      "Epoch 30/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5152 - accuracy: 0.7511 - val_loss: 0.5464 - val_accuracy: 0.7358\n",
      "Epoch 31/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5066 - accuracy: 0.7559 - val_loss: 0.5637 - val_accuracy: 0.7526\n",
      "Epoch 32/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.5064 - accuracy: 0.7603 - val_loss: 0.5570 - val_accuracy: 0.7358\n",
      "Epoch 33/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4971 - accuracy: 0.7651 - val_loss: 0.5536 - val_accuracy: 0.7326\n",
      "Epoch 34/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4992 - accuracy: 0.7624 - val_loss: 0.5644 - val_accuracy: 0.7211\n",
      "Epoch 35/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4923 - accuracy: 0.7703 - val_loss: 0.5629 - val_accuracy: 0.7253\n",
      "Epoch 36/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4894 - accuracy: 0.7654 - val_loss: 0.5577 - val_accuracy: 0.7453\n",
      "Epoch 37/100\n",
      "268/268 [==============================] - 3s 10ms/step - loss: 0.4915 - accuracy: 0.7714 - val_loss: 0.5675 - val_accuracy: 0.7358\n",
      "Epoch 38/100\n",
      "217/268 [=======================>......] - ETA: 0s - loss: 0.4783 - accuracy: 0.7764"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m y_train, y_val \u001b[38;5;241m=\u001b[39m y_train_full[train], y_train_full[val]\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Training and Evaluation\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpointer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val)\u001b[38;5;241m.\u001b[39mreshape(y_val\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],)\n\u001b[1;32m     38\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m (y_pred \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n",
      "File \u001b[0;32m/scratch/users/anup/condaenv/envs/succsite/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/scratch/users/anup/condaenv/envs/succsite/lib/python3.9/site-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/scratch/users/anup/condaenv/envs/succsite/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/scratch/users/anup/condaenv/envs/succsite/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/scratch/users/anup/condaenv/envs/succsite/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/scratch/users/anup/condaenv/envs/succsite/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/users/anup/condaenv/envs/succsite/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/scratch/users/anup/condaenv/envs/succsite/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/scratch/users/anup/condaenv/envs/succsite/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_k = 10\n",
    "kfold = KFold(n_k, shuffle=True)\n",
    "\n",
    "n_epo = 100\n",
    "batch_size = 32\n",
    "# initialize average variables\n",
    "avg_acc, avg_mcc, avg_sp, avg_sn = 0, 0, 0, 0\n",
    "\n",
    "for train, val in kfold.split(X_train_full_embedding, y_train_full):\n",
    "    \n",
    "    # Early stopping\n",
    "    es = EarlyStopping(monitor='val_accuracy', patience=7, mode='auto')\n",
    "\n",
    "    # Checkpointer\n",
    "    metric = 'val_accuracy'\n",
    "    checkpointer = ModelCheckpoint(filepath=\"models/transformer_st_model_best.h5\",\n",
    "                            monitor = metric,\n",
    "                            verbose=0, \n",
    "                            save_weights_only=False,\n",
    "                            save_best_only=True)\n",
    "\n",
    "    model = embedding_model() #CNN_Embedding()\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3),\n",
    "              loss=BinaryCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    # seperate val data\n",
    "    X_train, X_val = X_train_full_embedding[train], X_train_full_embedding[val]\n",
    "    y_train, y_val = y_train_full[train], y_train_full[val]\n",
    "\n",
    "    # Training and Evaluation\n",
    "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epo, verbose=1, callbacks=[checkpointer],\n",
    "                            validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "    y_pred = model.predict(X_val).reshape(y_val.shape[0],)\n",
    "\n",
    "    y_pred = (y_pred > 0.5)\n",
    "    y_pred = [int(i) for i in y_pred]\n",
    "    y_val = np.array(y_val)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    sn = cm[1][1]/(cm[1][1]+cm[1][0])\n",
    "    sp = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "    print(acc,mcc,sn,sp)\n",
    "    # plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9e2f5e-b67f-41e3-a8cc-13788627d349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aae53f7-6f6d-48a4-b433-0a7ff0b3831b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "gpu = tf.config.list_physical_devices('GPU')\n",
    "if len(gpu) > 0:\n",
    "    from numba import cuda\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e874a29-7438-4819-ba73-02888d07dbd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c982f8-9903-4fdc-a6dd-31f26f926b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56022520-5788-4213-b826-f5b3fbcdb664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
